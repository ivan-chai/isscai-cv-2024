{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c690063d-d586-4a6f-97c9-65fdbc3372f4",
   "metadata": {},
   "source": [
    "# Finetuning foundation models\n",
    "\n",
    "In this notebook we finetune ViT using different training strategies:\n",
    "* Head finetuning\n",
    "* Early stopping\n",
    "* LoRA\n",
    "\n",
    "**Goal.** The goal of this notebook is get practical skills in using pretrained transformer models.\n",
    "\n",
    "You need the following extra libraries beyond PyTorch:\n",
    "* torchvision\n",
    "* transformers\n",
    "* peft (another HuggingFace library that implements LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf464e4-0762-42de-a96c-6f6aa3c369ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install PyTorch Lightning and PEFT.\n",
    "# ! pip install pytorch_lightning peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1d3c9-70e7-4b27-8947-69072ef30b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torchvision\n",
    "import transformers\n",
    "import pytorch_lightning as pl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "MODEL = \"facebook/vit-mae-base\"\n",
    "\n",
    "def clean_memory():\n",
    "    to_remove = set()\n",
    "    for k, v in globals().items():\n",
    "        if isinstance(v, (torch.nn.Module, pl.LightningModule)):\n",
    "            to_remove.add(k)\n",
    "    for k in to_remove:\n",
    "        del globals()[k]\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde54d0-78c6-4a29-aa8f-99021b5efea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_resources():\n",
    "    torchvision.datasets.CIFAR100(root=\"cifar100\", train=True, download=True)\n",
    "    model = transformers.ViTMAEModel.from_pretrained(MODEL)\n",
    "    print(\"Num parameters:\", sum([p.numel() for p in model.parameters()]))\n",
    "\n",
    "fetch_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a981de-8aef-4aaf-b72e-5f1e5cffc890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(pl.LightningDataModule):\n",
    "    def __init__(self, num_workers=4, batch_size=16, transform=None):\n",
    "        super().__init__()\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        image_processor = transformers.AutoImageProcessor.from_pretrained(MODEL)\n",
    "        self.transform = lambda x: image_processor(images=x)[\"pixel_values\"][0]\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        dataset = torchvision.datasets.CIFAR100(root=\"cifar100\", train=True,\n",
    "                                                transform=self.transform)\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,  # The number of images in the batch.\n",
    "            num_workers=self.num_workers,  # The number of concurrent readers and preprocessors.\n",
    "            drop_last=True,  # Drop the truncated last batch during training.\n",
    "            pin_memory=torch.cuda.is_available(),  # Optimize CUDA data transfer.\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = torchvision.datasets.CIFAR100(root=\"cifar100\", train=False,\n",
    "                                                transform=self.transform)\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,  # The number of images in the batch.\n",
    "            num_workers=self.num_workers,  # The number of concurrent readers and preprocessors.\n",
    "            pin_memory=torch.cuda.is_available(),  # Optimize CUDA data transfer.\n",
    "        )\n",
    "\n",
    "def check_data():\n",
    "    data_module = Data()\n",
    "    x, y = next(iter(data_module.val_dataloader()))  # Val loader.\n",
    "    print(\"Images batch:\", x.shape, x.dtype)\n",
    "    print(\"Labels batch:\", y.shape, y.dtype)\n",
    "\n",
    "check_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f23ce4-a994-4ba4-b6b7-22174d1a2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(pl.LightningModule):\n",
    "    def __init__(self, model, lr):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).logits\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Skip freezed parameters.\n",
    "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.lr)\n",
    "\n",
    "    def common_step(self, batch):\n",
    "        images, labels = batch\n",
    "        logits = self(images)\n",
    "        loss = self.loss(logits, labels)\n",
    "        with torch.no_grad():\n",
    "            predictions = logits.argmax(1)  # (B).\n",
    "            correct = (predictions == labels).sum().item()\n",
    "            accuracy = correct / labels.numel()\n",
    "        return loss, accuracy\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss, accuracy = self.common_step(batch)\n",
    "        self.log(\"train/loss\", loss, prog_bar=True)\n",
    "        self.log(\"train/accuracy\", accuracy, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        loss, accuracy = self.common_step(batch)\n",
    "        self.log(\"val/loss\", loss, on_epoch=True)\n",
    "        self.log(\"val/accuracy\", accuracy, on_epoch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a9021-392e-4ac2-83f3-88311e251b4b",
   "metadata": {},
   "source": [
    "# Head fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9497de-7452-4f60-b6a1-12275d7a83f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084ac10-caad-4dcf-bc79-f07e9092b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_head():\n",
    "    model = transformers.ViTForImageClassification.from_pretrained(MODEL)\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    # Add more layers for better flexibility.\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        torch.nn.Linear(768, 256, bias=False),\n",
    "        torch.nn.BatchNorm1d(256),\n",
    "        torch.nn.ReLU(inplace=True),\n",
    "        torch.nn.Linear(256, 100)\n",
    "    )\n",
    "    # Use a typical learning rate for supervised training.\n",
    "    module = Module(model, lr=1e-4)\n",
    "    print(\"Num trainable parameters:\", sum([p.numel() for p in module.parameters() if p.requires_grad]))\n",
    "    trainer = pl.Trainer(max_epochs=10,\n",
    "                         precision=\"16-mixed\",\n",
    "                         logger=pl.loggers.TensorBoardLogger(\"lightning_logs\", name=\"HeadTuning\"))\n",
    "    # We can increase batch size, because the backbone doesn't require gradients and needs less memory.\n",
    "    trainer.fit(module, Data(batch_size=128))\n",
    "\n",
    "clean_memory()\n",
    "finetune_head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d63eb2b-4de4-43bd-8fd4-c7163d317cf4",
   "metadata": {},
   "source": [
    "# Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f39289-7370-40d9-9f1e-f9dae40ed98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a630d-434a-40b4-b212-54e042de337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping():\n",
    "    model = transformers.ViTForImageClassification.from_pretrained(MODEL)\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        torch.nn.Linear(768, 100)\n",
    "    )\n",
    "    module = Module(model, lr=2e-5)\n",
    "    print(\"Num trainable parameters:\", sum([p.numel() for p in module.parameters() if p.requires_grad]))\n",
    "    trainer = pl.Trainer(max_epochs=20,\n",
    "                         precision=\"16-mixed\",\n",
    "                         logger=pl.loggers.TensorBoardLogger(\"lightning_logs\", name=\"EarlyStopping Long\"))\n",
    "    trainer.fit(module, Data())\n",
    "\n",
    "clean_memory()\n",
    "early_stopping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f3c77d-3d4c-49e6-b0e0-6440763b53e4",
   "metadata": {},
   "source": [
    "# LoRA finetuning\n",
    "\n",
    "Train a low-rank weight update. The transformers library provides necessary tools.\n",
    "\n",
    "LoRA updates the weights tensor as:\n",
    "\n",
    "```\n",
    "scaling = alpha / r\n",
    "weight += (lora_B @ lora_A) * scaling \n",
    "```\n",
    "\n",
    "The main parameters are:\n",
    "* the scaling weight ```alpha```\n",
    "* the factorization rank ```r```\n",
    "* the list of modules to update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1526d43-2d34-4187-85ea-093e625bebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854616d0-d40c-4588-85ab-c93b52d8d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "\n",
    "def finetune_lora():\n",
    "    model = transformers.ViTForImageClassification.from_pretrained(MODEL)\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        torch.nn.Linear(768, 100)\n",
    "    )\n",
    "    print(model)\n",
    "    config = peft.LoraConfig(r=128,\n",
    "                             lora_alpha=256,\n",
    "                             target_modules=[\"query\", \"value\"],  # Modules to train with LoRA.\n",
    "                             modules_to_save=[\"classifier\"])  # Modules to train without LoRA.\n",
    "    model = peft.get_peft_model(model, config)\n",
    "    model.print_trainable_parameters()\n",
    "    module = Module(model, lr=2e-5)\n",
    "    print(\"Num trainable parameters:\", sum([p.numel() for p in module.parameters() if p.requires_grad]))\n",
    "    trainer = pl.Trainer(max_epochs=2,\n",
    "                         precision=\"16-mixed\",\n",
    "                         logger=pl.loggers.TensorBoardLogger(\"lightning_logs\", name=\"LoRA (lr 2e-5, QV, r128)\"))\n",
    "    trainer.fit(module, Data())\n",
    "    model = model.merge_and_unload()\n",
    "\n",
    "clean_memory()\n",
    "finetune_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992537f6-c5a0-47ad-935d-1deb89e65b25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
