{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6f32a7-fd0a-45d8-b940-42f1c924f0c9",
   "metadata": {},
   "source": [
    "# Segmentation with UNet\n",
    "\n",
    "In this notebook we will train a simple UNet model for foreground/background segmentation.\n",
    "\n",
    "**Goal.** The goal of this notebook is to get experience in working with pix-to-pix models and training segmentation models.\n",
    "\n",
    "You need the following extra libraries beyond PyTorch:\n",
    "* albumentations\n",
    "* torchvision\n",
    "* Pillow (PIL)\n",
    "* (optional) segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e5e7b1-dcab-48e5-908d-ae4f654ad733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import gc\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "import torch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "\n",
    "print(\"Have CUDA:\", torch.cuda.is_available())\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 8\n",
    "VIZ_IMAGES = 4\n",
    "\n",
    "DATA_ROOT = \".\"\n",
    "VOC_YEAR = \"2012\"\n",
    "LABELS = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
    "          \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\",\n",
    "          \"motorbike\", \"person\", \"potted_plant\", \"sheep\", \"sofa\", \"train\", \"tv/monitor\"]\n",
    "\n",
    "# Helper tools.\n",
    "# You can skip this block.\n",
    "\n",
    "class Module(pl.LightningModule):\n",
    "    def __init__(self, model, loss):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images, masks = batch\n",
    "        predictions = self(images)\n",
    "        loss = self.loss(predictions, masks)\n",
    "        self.log(\"train/loss\", loss, prog_bar=True)\n",
    "        if self.global_step % 100 == 0:\n",
    "            # Log sample images.\n",
    "            s_pred, s_image, s_mask = predictions[:VIZ_IMAGES], images[:VIZ_IMAGES], masks[:VIZ_IMAGES]\n",
    "            s_image = s_image - s_image.min()\n",
    "            s_image = s_image / s_image.max()  # (B, C, H, W).\n",
    "            s_mask = s_mask.unsqueeze(1).repeat(1, 3, 1, 1)  # (B, C, H, W).\n",
    "            s_pred = torch.sigmoid(s_pred).repeat(1, 3, 1, 1)  # (B, C, H, W).\n",
    "            s_pred_bin = (s_pred > 0.5).float()\n",
    "            log_image = torch.cat([s_image, s_mask, s_pred, s_pred_bin], dim=2).permute(1, 2, 0, 3).flatten(2, 3)  # (C, 4H, BW).\n",
    "            for logger in self.trainer.loggers:\n",
    "                if isinstance(logger, pl.loggers.TensorBoardLogger):\n",
    "                    tb_logger = logger.experiment\n",
    "                    tb_logger.add_image(f\"Result\", log_image, self.global_step)\n",
    "            self._data = [s_image, s_mask, s_pred, s_pred_bin, predictions[:4]]\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, masks = batch\n",
    "        predictions = self(images)\n",
    "        loss = self.loss(predictions, masks)\n",
    "        self.log(\"val/loss\", loss, on_epoch=True)\n",
    "        # Compute Jaccard index.\n",
    "        assert predictions.shape[1] == 1\n",
    "        pred_masks = predictions.squeeze(1) > 0  # (B, H, W).\n",
    "        assert pred_masks.shape == masks.shape\n",
    "        intersection = torch.logical_and(pred_masks, masks).sum()\n",
    "        union = pred_masks.sum() + masks.sum() - intersection\n",
    "        jaccard_index = intersection / union.clip(min=1)\n",
    "        self.log(\"val/Jaccard\", jaccard_index, on_epoch=True)\n",
    "\n",
    "\n",
    "def show_segmentations(model, dataset):\n",
    "    model.eval()\n",
    "    for _ in range(VIZ_IMAGES):\n",
    "        image, mask_gt = dataset[random.randint(0, len(dataset) - 1)]\n",
    "        with torch.no_grad():\n",
    "            predicted = model(image.unsqueeze(0))[0]  # CHW.\n",
    "            predicted = torch.sigmoid(predicted[0])\n",
    "            predicted_labels = predicted > 0.5\n",
    "        predicted_labels = predicted_labels.cpu().numpy().astype(np.uint8)\n",
    "        image = image.permute(1, 2, 0)\n",
    "        image = image - image.min()\n",
    "        image = image / image.max()\n",
    "        mask = (predicted_labels > 0)[..., None]\n",
    "        selected = image * mask + 255 * (1 - mask)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 5, figsize=(12, 5))\n",
    "        axs[0].imshow(image)\n",
    "        axs[1].imshow(mask_gt)\n",
    "        axs[2].imshow(predicted)\n",
    "        axs[3].imshow(predicted_labels)\n",
    "        axs[4].imshow(selected)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def clean_memory():\n",
    "    to_remove = set()\n",
    "    for k, v in globals().items():\n",
    "        if isinstance(v, (torch.nn.Module, pl.LightningModule)):\n",
    "            to_remove.add(k)\n",
    "    for k in to_remove:\n",
    "        del globals()[k]\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "class CheckerError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def check_conv1x1(layer_fn):\n",
    "    layer = layer_fn(2, 5)\n",
    "    if (not isinstance(layer, torch.nn.Sequential)) or (len(layer) != 2):\n",
    "        raise CheckerError(\"conv1x1: Need an nn.Sequential module with 2 layers.\")\n",
    "    conv = layer[0]\n",
    "    bn = layer[1]\n",
    "    if (not isinstance(conv, torch.nn.Conv2d)) or (not isinstance(bn, torch.nn.BatchNorm2d)):\n",
    "        raise CheckerError(\"conv1x1: The first layer must be convolution and the second layer must be batch normalization.\")\n",
    "    if conv.kernel_size != (1, 1):\n",
    "        raise CheckerError(\"conv1x1: wrong kernel size\")\n",
    "    if conv.in_channels != 2 or conv.out_channels != 5:\n",
    "        raise CheckerError(\"conv1x1: wrong channels\")\n",
    "    if conv.bias is not None:\n",
    "        raise CheckerError(\"conv1x1: Don't use bias before batch norm.\")\n",
    "    x = torch.randn(3, 2, 7, 9)\n",
    "    try:\n",
    "        layer(x)\n",
    "    except Exception:\n",
    "        raise CheckerError(\"conv1x1: Inference error\")\n",
    "\n",
    "\n",
    "def check_conv3x3(layer_fn):\n",
    "    layer = layer_fn(2, 5)\n",
    "    if (not isinstance(layer, torch.nn.Sequential)) or (len(layer) != 2):\n",
    "        raise CheckerError(\"conv3x3: Need an nn.Sequential module with 2 layers.\")\n",
    "    conv = layer[0]\n",
    "    bn = layer[1]\n",
    "    if (not isinstance(conv, torch.nn.Conv2d)) or (not isinstance(bn, torch.nn.BatchNorm2d)):\n",
    "        raise CheckerError(\"conv3x3: The first layer must be convolution and the second layer must be batch normalization.\")\n",
    "    if conv.kernel_size != (3, 3):\n",
    "        raise CheckerError(\"conv3x3: wrong kernel size\")\n",
    "    if conv.padding != (1, 1):\n",
    "        raise CheckerError(\"conv3x3: need SAME padding\")\n",
    "    if conv.in_channels != 2 or conv.out_channels != 5:\n",
    "        raise CheckerError(\"conv3x3: wrong channels\")\n",
    "    if conv.bias is not None:\n",
    "        raise CheckerError(\"conv3x3: Don't use bias before batch norm.\")\n",
    "    x = torch.randn(3, 2, 7, 9)\n",
    "    try:\n",
    "        layer(x)\n",
    "    except Exception:\n",
    "        raise CheckerError(\"conv3x3: Inference error\")\n",
    "\n",
    "\n",
    "def check_upsampling(layer_fn):\n",
    "    layer = layer_fn(scale_factor=2)\n",
    "    if not isinstance(layer, torch.nn.Upsample):\n",
    "        raise CheckerError(\"upsample: Need upsampling layer.\")\n",
    "    if layer.scale_factor != 2:\n",
    "        raise CheckerError(\"upsample: Wrong scale factor.\")\n",
    "    x = torch.randn(3, 2, 7, 9)\n",
    "    try:\n",
    "        layer(x)\n",
    "    except Exception:\n",
    "        raise CheckerError(\"upsample: Inference error\")\n",
    "\n",
    "\n",
    "def check_bce_loss(loss_class):\n",
    "    loss_computer = loss_class()\n",
    "    def to_logits(probs):\n",
    "        return np.log(probs) - np.log(1 - probs)\n",
    "    logits = torch.from_numpy(to_logits(np.array([0.9, 0.2]))).reshape(2, 1, 1, 1).float()\n",
    "    labels = torch.from_numpy(np.array([1, 0])).reshape(2, 1, 1).long()\n",
    "    value = loss_computer(logits, labels).item()\n",
    "    if abs(value + np.mean(np.log([0.9, 0.8]))) > 1e-5:\n",
    "        raise CheckerError(\"Wrong BCE loss\")\n",
    "\n",
    "\n",
    "def check_focal_loss(loss_class):\n",
    "    loss_computer = loss_class()\n",
    "    def to_logits(probs):\n",
    "        return np.log(probs) - np.log(1 - probs)\n",
    "    p = np.array([0.9, 0.2])\n",
    "    logits = torch.from_numpy(to_logits(p)).reshape(2, 1, 1, 1).float()\n",
    "    labels = torch.from_numpy(np.array([1, 0])).reshape(2, 1, 1).bool()\n",
    "    value = loss_computer(logits, labels).item()\n",
    "    gt_1 = - 0.01 * np.log(0.9)\n",
    "    gt_2 = - 0.04 * np.log(0.8)\n",
    "    gt = 0.5 * (gt_1 + gt_2)\n",
    "    if abs(value - gt) > 1e-5:\n",
    "        raise CheckerError(\"Wrong Focal Loss\")\n",
    "\n",
    "\n",
    "def check_dice_loss(loss_class):\n",
    "    loss_computer = loss_class()\n",
    "    def to_logits(probs):\n",
    "        return np.log(probs) - np.log(1 - probs)\n",
    "    p = np.array([0.9, 0.2])\n",
    "    logits = torch.from_numpy(to_logits(p)).reshape(2, 1, 1, 1).float()\n",
    "    labels = torch.from_numpy(np.array([1, 0])).reshape(2, 1, 1).bool()\n",
    "    value = loss_computer.single_class_dice_loss(logits, labels).item()\n",
    "    gt = -np.log(2 * 0.9 / (1.1 + 1))\n",
    "    if abs(value - gt) > 1e-5:\n",
    "        raise CheckerError(\"Wrong Dice Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45398b94-7435-40b8-a776-71b4dc64ae6b",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We use Pascal VOC dataset. Masks contain two special values: 0 for background and 255 for a countour. We will remove contour during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f7e2d-8cc2-4ee6-a1fd-bc5d1ae54f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download.\n",
    "VOCSegmentation(DATA_ROOT, VOC_YEAR, \"train\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b35bd-418e-4f21-b60f-9c9b585587e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valset_raw = VOCSegmentation(DATA_ROOT, VOC_YEAR, \"val\")\n",
    "\n",
    "image, mask = valset_raw[random.randint(0, len(valset_raw) - 1)]\n",
    "print(\"Mask values:\", set(np.array(mask).flatten().tolist()))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(image)\n",
    "axs[1].imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381d458-38a9-495b-90f1-8c6d49771ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(11)\n",
    "for _ in range(5):\n",
    "    i = random.randint(0, len(valset_raw) - 1)\n",
    "    print(\"Image {:05d} size: {}\".format(i, valset_raw[i][0].size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ac54a-2cf1-4233-a897-28fc0eb60da7",
   "metadata": {},
   "source": [
    "Images have different sizes, sometimes the longest side is less than 500 pixels. To train the model we must handle this problem using one of the following approaches:\n",
    "1. Scale images to a fixed size (change aspect ratios when necessary).\n",
    "2. Scale images to a fixed size by keeping aspect ratio (with padding).\n",
    "3. Scale images to a fixed size by cropping central parts with the required aspect ratio.\n",
    "4. Use the models capable of working with different image sizes.\n",
    "\n",
    "Note:\n",
    "- While UNet can work with images of different size, we can't make batch of them, and the training will be slow.\n",
    "- UNet is a convolutional network and each output depends only on the neighborhood pixels.\n",
    "\n",
    "We will apply the second option and the mirror padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbefec6f-2953-4033-8e0a-4d7cce091d49",
   "metadata": {},
   "source": [
    "### Data module\n",
    "\n",
    "We will use Albumentations to augment both [images and masks](https://albumentations.ai/docs/examples/example_kaggle_salt/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d25ac-39bc-41b3-adc6-a6a16cb46c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToBinary(A.DualTransform):\n",
    "    \"\"\"A custom Albumentations transform that converts multiclass masks to foreground / background.\"\"\"\n",
    "    \n",
    "    def __init__(self, background_label=0):\n",
    "        super().__init__(p=1)  # Always apply.\n",
    "        self.background_label = background_label\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "       return img\n",
    "        \n",
    "    def apply_to_mask(self, mask, **params):\n",
    "        new_mask = mask != self.background_label\n",
    "        return new_mask\n",
    "\n",
    "\n",
    "class TransformedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Apply Albumentations transform to the dataset.\"\"\"\n",
    "    def __init__(self, dataset, transform):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, mask = self.dataset[index]\n",
    "        transformed = self.transform(image=np.asarray(image), mask=np.asarray(mask))\n",
    "        return transformed[\"image\"], transformed[\"mask\"]\n",
    "\n",
    "\n",
    "class Data(pl.LightningDataModule):\n",
    "    \"\"\"Dataset class.\n",
    "    \n",
    "    Resize an image to the specified size keeping aspect ratio and fill\n",
    "    borders with reflections.\n",
    "\n",
    "    Args:\n",
    "        size: Output image size.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, batch_size=BATCH_SIZE, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        train_transform = A.Compose([\n",
    "            A.LongestMaxSize(size),\n",
    "            A.PadIfNeeded(size, size, position=\"center\"),\n",
    "            A.HorizontalFlip(),  # Train only.\n",
    "            A.RandomResizedCrop((size, size), scale=(0.7, 1.0)),  # Train only.\n",
    "            A.Normalize(),\n",
    "            ToBinary(),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        val_transform = A.Compose([\n",
    "            A.LongestMaxSize(size),\n",
    "            A.PadIfNeeded(size, size, position=\"center\"),\n",
    "            A.Normalize(),\n",
    "            ToBinary(),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        self.trainset = TransformedDataset(\n",
    "            VOCSegmentation(DATA_ROOT, VOC_YEAR, \"train\"),\n",
    "            train_transform\n",
    "        )\n",
    "        self.valset = TransformedDataset(\n",
    "            VOCSegmentation(DATA_ROOT, VOC_YEAR, \"val\"),\n",
    "            val_transform\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.trainset,\n",
    "            batch_size=self.batch_size,  # The number of images in the batch.\n",
    "            num_workers=self.num_workers,  # The number of concurrent readers and preprocessors.\n",
    "            drop_last=True,  # Drop the truncated last batch during training.\n",
    "            pin_memory=torch.cuda.is_available(),  # Optimize CUDA data transfer.\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.valset,\n",
    "            batch_size=self.batch_size,  # The number of images in the batch.\n",
    "            num_workers=self.num_workers,  # The number of concurrent readers and preprocessors.\n",
    "            pin_memory=torch.cuda.is_available(),  # Optimize CUDA data transfer.\n",
    "        )\n",
    "\n",
    "data = Data(IMAGE_SIZE)\n",
    "print(\"Train set size:\", len(data.trainset))\n",
    "print(\"Validation set size:\", len(data.valset))\n",
    "\n",
    "image, mask = data.valset[5]\n",
    "print(f\"Image dtype: {image.dtype}, image shape: {image.shape}\")\n",
    "print(f\"Mask dtype: {mask.dtype}, mask shape: {mask.shape}\")\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "image = image - image.min()\n",
    "image = image / image.max()\n",
    "axs[0].imshow(image.permute(1, 2, 0))\n",
    "axs[1].imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b166c-7c9b-4e4d-9aa7-eb271d36940c",
   "metadata": {},
   "source": [
    "Not that the mask is transformed similar to the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad0d2f-ab54-49b7-b0cc-14fb09dc2c2f",
   "metadata": {},
   "source": [
    "# UNet\n",
    "\n",
    "In this block we will implement a model similar to [UNet](https://arxiv.org/pdf/1505.04597.pdf).\n",
    "\n",
    "UNet is composed of encoder and decoder. Encoder computes embedding, while decoder reconstructs the image. Decoder has a structure similar to the reversed encoder. The main differences between encoder and decoder is that encoder applies max pooling or a stride to decrease image dimensions, while decoder exploits upsampling layers or transposed convolutions. Anyway, the hidden activations of encoder and decoder layers have similar shapes, and we can append encoder activations to decoder layers inputs. This way, the information about an input image will be passed to the decoder with minimum loss, and the mask will be fine.\n",
    "\n",
    "<img src=\"example-image.jpg\" align=\"left\" hspace=\"20\" width=\"20%\" height=\"20%\"/> \n",
    "<img src=\"u-net.jpg\" align=\"left\" hspace=\"20\" width=\"50%\" height=\"50%\"/> \n",
    "<img src=\"example-mask.jpg\" align=\"left\" hspace=\"20\" width=\"20%\" height=\"20%\"/> \n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "We will use a custom implementation of UNet to speedup training and improve quality:\n",
    "* Image size 256 instead of 572\n",
    "* Add residual connections, as in ResNet\n",
    "* Add Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f3ce2-93ba-40a2-9716-2cdc146f021c",
   "metadata": {},
   "source": [
    "### Assignment 1. Implement basic blocks.\n",
    "\n",
    "Cheat sheet:\n",
    "\n",
    "```python\n",
    "torch.nn.Sequential(*args: Module)\n",
    "\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "\n",
    "torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)\n",
    "\n",
    "torch.nn.ReLU(inplace=False)\n",
    "\n",
    "torch.nn.Upsample(size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ac7f3-bfab-471c-9676-d066d29c7473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conv1x1_bn(in_channels, out_channels, stride=1):\n",
    "    \"\"\"Create a pair of a convolutional layer with kernel size 1 and batch normalization layer.\"\"\"\n",
    "    \n",
    "    return torch.nn.Sequential(\n",
    "        # Put your code here.\n",
    "        torch.nn.Conv2d(in_channels, out_channels, 1, bias=False, stride=stride),\n",
    "        torch.nn.BatchNorm2d(out_channels)\n",
    "    )\n",
    "\n",
    "\n",
    "def make_conv3x3_bn(in_channels, out_channels, bias=True, stride=1):\n",
    "    \"\"\"Create a pair of a convolutional layer with the SAME padding and batch normalization layer.\"\"\"\n",
    "\n",
    "    return torch.nn.Sequential(\n",
    "        # Put your code here.\n",
    "        torch.nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False, stride=stride),\n",
    "        torch.nn.BatchNorm2d(out_channels)\n",
    "    )\n",
    "\n",
    "\n",
    "def make_upspampling(scale_factor=2):\n",
    "    \"\"\"Create an upsampling layer. Choose 'bilinear' upsampling mode.\"\"\"\n",
    "    \n",
    "    # Your code starts here.\n",
    "    \n",
    "    layer = torch.nn.Upsample(scale_factor=scale_factor, mode=\"bilinear\")\n",
    "    \n",
    "    # The end of your code.\n",
    "    \n",
    "    return layer\n",
    "\n",
    "def make_activation():\n",
    "    \"\"\"Make a ReLU layer. It's better to compute inplace.\"\"\"\n",
    "\n",
    "    # Your code starts here.\n",
    "    \n",
    "    layer = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "    # The end of your code.\n",
    "\n",
    "    return layer\n",
    "\n",
    "check_conv1x1(make_conv1x1_bn)\n",
    "check_conv3x3(make_conv3x3_bn)\n",
    "check_upsampling(make_upspampling)\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66cc46-cc14-4d2f-a0e2-4078cf11ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResConvBlock(torch.nn.Module):\n",
    "    \"\"\"A simple convolutional block with a residual connection.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.main_path = torch.nn.Sequential(\n",
    "            make_conv3x3_bn(in_channels, out_channels, stride=stride),\n",
    "            make_activation(),\n",
    "            make_conv3x3_bn(out_channels, out_channels)\n",
    "        )\n",
    "        self.residual_path = make_conv1x1_bn(in_channels, out_channels, stride=stride)\n",
    "        self.last_relu = make_activation()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        result = self.main_path(x)\n",
    "        residual = self.residual_path(x)\n",
    "        result = self.last_relu(result + residual)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aed243-4b5e-468d-bcf1-6d5306243408",
   "metadata": {},
   "source": [
    "UNet is composed of two branches:\n",
    "* input convolutions\n",
    "* encoder\n",
    "* decoder\n",
    "* final projection\n",
    "\n",
    "`Input convolutions` is a simple transform for image preprocessing. Encoder and decoder branches include multiple blocks. `UNetDown` class implements a computation block between two downsampling layers, starting from a strided convolution. Similarly, `UNetUp` implements a computation block between two upsampling layers, starting from upsampling. The `final projection` is a simple 1x1 convolution that extracts the required number of channels from the decoder output.\n",
    "\n",
    "The final model structure:\n",
    "```\n",
    "input -> conv -> conv -> Down x 4 -> Up x 4 -> conv -> output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4146bf59-06a5-4ff1-88e8-5e0a4088b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDown(torch.nn.Sequential):\n",
    "    \"\"\"A computation block between two downsampling layers, starting from a strided convolution.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, block):\n",
    "        layers = [\n",
    "            block(in_channels, out_channels, stride=2)\n",
    "        ]\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class UNetUp(torch.nn.Module):\n",
    "    \"\"\"A computation block between two upsampling layers, starting from upsampling.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, block):\n",
    "        super().__init__()\n",
    "        self.upsample = make_upspampling(scale_factor=2)\n",
    "        self.layer = block(in_channels + out_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x_down, x_up):\n",
    "        x_up = self.upsample(x_up)\n",
    "        x = torch.cat((x_down, x_up), axis=1)  # Concatenate along channels dim.\n",
    "        result = self.layer(x)\n",
    "        return result\n",
    "\n",
    "\n",
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes=1, num_scales=4, base_filters=64, block=ResConvBlock):\n",
    "        \"\"\"Create UNet.\n",
    "        \n",
    "        Args\n",
    "            num_classes: The number of output logits.\n",
    "            num_scales: The number of downsampling and upsampling layers.\n",
    "            base_filters: The number of filters of the first convolution. All other channels are relative to this value.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_convolutions = block(3, base_filters)\n",
    "        \n",
    "        layers = []\n",
    "        filters = base_filters\n",
    "        layers.append(UNetDown(filters, filters, block))\n",
    "        for i in range(num_scales - 1):\n",
    "            layers.append(UNetDown(filters, filters * 2, block))\n",
    "            filters *= 2\n",
    "        self.down_layers = torch.nn.Sequential(*layers)\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_scales - 1):\n",
    "            layers.append(UNetUp(filters, filters // 2, block))\n",
    "            filters //= 2\n",
    "        layers.append(UNetUp(filters, filters, block))\n",
    "        self.up_layers = torch.nn.Sequential(*layers)\n",
    "        \n",
    "        self.output_convolution = torch.nn.Conv2d(filters, num_classes, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        down_results = [self.input_convolutions(x)]\n",
    "        for layer in self.down_layers:\n",
    "            down_results.append(layer(down_results[-1]))\n",
    "        x = down_results[-1]\n",
    "        for i, layer in enumerate(self.up_layers):\n",
    "            x = layer(down_results[-2 - i], x)\n",
    "        x = self.output_convolution(x)\n",
    "        return x\n",
    "\n",
    "unet = UNet()\n",
    "\n",
    "print(\"Output shape:\", unet(next(iter(data.train_dataloader()))[0]).shape)\n",
    "\n",
    "print(unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38ae2d8-32ee-4061-985e-a560a4036b64",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752a63f0-4332-47e7-8c37-0cd1742f8a85",
   "metadata": {},
   "source": [
    "### Assignment 2. Implement BCE loss for segmentation.\n",
    "Cheat sheet:\n",
    "\n",
    "```python\n",
    "torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10033c7b-0cf6-4e42-82e9-5fd1175ea8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCELoss(torch.nn.Module):       \n",
    "    def __call__(self, predicted, masks):\n",
    "        if predicted.shape[1] != 1:\n",
    "            raise ValueError(\"Need binary predictions\")\n",
    "        predicted = predicted.squeeze(1)\n",
    "    \n",
    "        # predicted: float32, BHW.\n",
    "        # masks: bool, BHW.\n",
    "            \n",
    "        # Ваш код здесь.\n",
    "        \n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(predicted, masks.float())\n",
    "        \n",
    "        # Конец вашего кода.\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "check_bce_loss(BCELoss)\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d83e9c1-034d-4d0d-ae1b-0c49424520d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3401688-623e-425b-9a3c-6174f8351ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "\n",
    "model = Module(UNet(), BCELoss())\n",
    "trainer = pl.Trainer(max_epochs=10,\n",
    "                     logger=pl.loggers.TensorBoardLogger(\"lightning_logs\", name=\"BCE\"))\n",
    "trainer.fit(model, data)\n",
    "show_segmentations(model, data.valset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c7c6d-5500-4020-afef-b2d71ff0083d",
   "metadata": {},
   "source": [
    "### Assignment 3. Implement Focal Loss\n",
    "\n",
    "For positive class (mask = 1): $loss = -(1 - p_+)^\\gamma \\log p_+$\n",
    "\n",
    "For negative class (mask = 0): $loss = -(1 - p_-)^\\gamma \\log p_-$\n",
    "\n",
    "Where $p_+$ is the predicted probability and $p_+ + p_- = 1$.\n",
    "\n",
    "Note, that the model predicts logits rather than probabilies. Use *sigmoid* activation and `average` aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda033d2-4467-4694-b962-45f109cda85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss():\n",
    "    def __init__(self, gamma=2):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, predicted, masks):\n",
    "        if predicted.shape[1] != 1:\n",
    "            raise ValueError(\"{} can't be applied to a multi-class problem\".format(type(self)))\n",
    "        predicted = predicted.squeeze(1)\n",
    "            \n",
    "        # predicted: float32, BHW.\n",
    "        # masks: bool, BHW.\n",
    "\n",
    "        # Your code starts here.\n",
    "        p = torch.sigmoid(predicted)  # (B, H, W).\n",
    "        np = 1 - p\n",
    "        log_p = torch.nn.functional.logsigmoid(predicted)\n",
    "        log_np = torch.nn.functional.logsigmoid(-predicted)\n",
    "        pos_term = (np ** self.gamma) * log_p\n",
    "        neg_term = (p ** self.gamma) * log_np\n",
    "        loss = -torch.where(masks, pos_term, neg_term).mean()\n",
    "        \n",
    "        # The end of your code.\n",
    "        \n",
    "        return loss\n",
    "\n",
    "check_focal_loss(FocalLoss)\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db10aa2-f343-4e82-b33b-504429a57c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479fdb47-6779-4445-be16-e460f42356e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "\n",
    "model = Module(UNet(), FocalLoss())\n",
    "trainer = pl.Trainer(max_epochs=10,\n",
    "                     logger=pl.loggers.TensorBoardLogger(\"lightning_logs\", name=\"FocalLoss\"))\n",
    "trainer.fit(model, data)\n",
    "show_segmentations(model, data.valset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199fd39-5421-4c16-b197-5ddf2e1b5eed",
   "metadata": {},
   "source": [
    "### Assignment 4. Implement Dice Loss\n",
    "\n",
    "$\n",
    "\\mathrm{DiceLoss} = -\\log\\frac{2\\sum\\limits_{i,j} x_{i,j} y_{i,j}}{\\sum\\limits_{i,j} x_{i, j} + y_{i,j}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3347968c-14b7-42d3-8590-4ba61b3dbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss():\n",
    "    def __init__(self, focal_gamma=2):\n",
    "        self.focal = FocalLoss(gamma=focal_gamma)\n",
    "        \n",
    "    def single_class_dice_loss(self, predicted, masks):\n",
    "        if predicted.shape[1] != 1:\n",
    "            raise ValueError(\"{} can't be applied to a multi-class problem\".format(type(self)))\n",
    "        predicted = predicted.squeeze(1)\n",
    "            \n",
    "        # predicted: float32, BHW.\n",
    "        # masks: bool, BHW.\n",
    "\n",
    "        # Your code starts here.\n",
    "        masks = masks.float()\n",
    "        p = torch.sigmoid(predicted)  # (B, H, W).\n",
    "        double_intersection = 2 * (p * masks).sum()\n",
    "        area_sum = p.sum() + masks.sum()\n",
    "        loss = - double_intersection.clip(min=1e-6).log() + area_sum.clip(min=1e-6).log()\n",
    "        \n",
    "        # The end of your code.\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def __call__(self, predicted, masks):\n",
    "        positive = self.single_class_dice_loss(predicted, masks)\n",
    "        negative = self.single_class_dice_loss(-predicted, ~masks)\n",
    "        return 0.5 * (positive + negative) + self.focal(predicted, masks)\n",
    "\n",
    "check_dice_loss(DiceLoss)\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da81e0a-e312-458a-93fa-75f6413a6979",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c981e574-2737-44ea-aca6-a1ebd37009a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "\n",
    "model = Module(UNet(), DiceLoss())\n",
    "trainer = pl.Trainer(max_epochs=10,\n",
    "                     logger=pl.loggers.TensorBoardLogger(\"lightning_logs\", name=\"Dice\"))\n",
    "trainer.fit(model, data)\n",
    "show_segmentations(model, data.valset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24123e4-59c3-45d4-9f4b-636be0059561",
   "metadata": {},
   "source": [
    "# Use pretrained model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c64a02-e6c3-4a14-a106-90b5116a1205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0a4c3-3742-4781-a68a-cc74607ef2f2",
   "metadata": {},
   "source": [
    "### Before tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbcef0-2b20-41e5-b678-c15fbcf6582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_pretrained = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", classes=1)\n",
    "show_segmentations(unet_pretrained, data.valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eb9a4a-bf92-468c-a227-0adc685e589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e04a4-3412-4bb5-aff9-5653f54d7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "\n",
    "unet_pretrained = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", classes=1)\n",
    "model = Module(unet_pretrained, DiceLoss())\n",
    "trainer = pl.Trainer(max_epochs=10,\n",
    "                     logger=pl.loggers.TensorBoardLogger(\"lightning_logs\", name=\"Pretrained\"))\n",
    "trainer.fit(model, data)\n",
    "show_segmentations(unet_pretrained, data.valset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff71fdf-85b7-4684-8b17-991f8611667e",
   "metadata": {},
   "source": [
    "# Summary\n",
    "* UNet architecture predicts fine-grained masks\n",
    "* All loss functions have similar quality in terms of Jaccard Index\n",
    "* One possible reason is low class imbalance in the selected problem\n",
    "* Pretraining largely affects the final quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a74f95d-ba2c-4571-8af0-ea3c3ecb486b",
   "metadata": {},
   "source": [
    "# Homework (optional)\n",
    "\n",
    "Try to implement multi-class segmentation on the same dataset. Is it possible to achive a good quality without pretraining?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
