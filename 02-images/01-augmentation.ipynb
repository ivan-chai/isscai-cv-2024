{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5221e2b8-abe4-4afe-889a-2765f4121f77",
   "metadata": {},
   "source": [
    "# Image augmentations\n",
    "\n",
    "In this notebook we will apply popular augmentation techniques and reduce overfitting\n",
    "\n",
    "**Goal.** The goal of this notebook is to develop the basic skills in image processing.\n",
    "\n",
    "You need the following extra libraries beyond PyTorch:\n",
    "* torchvision\n",
    "* opencv-python (cv2)\n",
    "* Pillow (PIL)\n",
    "* (optional) albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7989078-a2a1-4287-a6a9-954f528ab39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install PyTorch Lightning.\n",
    "# ! pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef87a704-82e1-4ffc-b9f1-7bcb54dd3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Below are helper tools. You can skip this block.\n",
    "\n",
    "DATA_ROOT = \"cifar10\"\n",
    "\n",
    "def show_images_dataset(dataset, n=5, collate_fn=lambda pair: pair[0]):\n",
    "    images = [collate_fn(random.choice(dataset)) for _ in range(n)]\n",
    "    grid = torchvision.utils.make_grid(images)\n",
    "    grid -= grid.min()\n",
    "    grid /= grid.max()\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "def show_augmenter_results(augmenter):\n",
    "    trainset_notransform = torchvision.datasets.CIFAR10(DATA_ROOT, train=True, download=True)\n",
    "    pil_image = random.choice(trainset_notransform)[0]\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        augmenter,\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    show_images_dataset([pil_image], collate_fn=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3f96a4-376c-43aa-bc90-3fafeea40d7f",
   "metadata": {},
   "source": [
    "# Simple training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b79973-ca24-4ed9-b9c2-c0ea7c62d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(pl.LightningDataModule):\n",
    "    def __init__(self, num_workers=4, batch_size=32, augmenter=None):\n",
    "        super().__init__()\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        basic_transforms = [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "        ]\n",
    "        # Apply augmentation to the training set only.\n",
    "        train_transforms = [augmenter] if augmenter is not None else []\n",
    "        self.train_transform = torchvision.transforms.Compose(train_transforms + basic_transforms)\n",
    "        self.test_transform = torchvision.transforms.Compose(basic_transforms)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        dataset = torchvision.datasets.CIFAR10(root=DATA_ROOT, train=True, download=True,\n",
    "                                               transform=self.train_transform)\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,  # The number of images in the batch.\n",
    "            num_workers=self.num_workers,  # The number of concurrent readers and preprocessors.\n",
    "            drop_last=True,  # Drop the truncated last batch during training.\n",
    "            pin_memory=torch.cuda.is_available(),  # Optimize CUDA data transfer.\n",
    "        )\n",
    "\n",
    "    # Validate on test.\n",
    "    def val_dataloader(self):\n",
    "        return self.test_dataloader()\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = torchvision.datasets.CIFAR10(root=\"cifar10\", train=False, download=True,\n",
    "                                               transform=self.test_transform)\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,  # The number of images in the batch.\n",
    "            num_workers=self.num_workers,  # The number of concurrent readers and preprocessors.\n",
    "            pin_memory=torch.cuda.is_available(),  # Optimize CUDA data transfer.\n",
    "        )\n",
    "\n",
    "data_module = Data()\n",
    "x, y = next(iter(data_module.test_dataloader()))  # Test loader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a973f-3fca-42ba-8d21-b5269025d9fe",
   "metadata": {},
   "source": [
    "**Model.** We will use a standard ResNet implementation from torchvision. Other popular repositories with vision models include:\n",
    "1. [transformers](https://huggingface.co/docs/transformers/index)\n",
    "2. [pretrained-models](https://github.com/cadene/pretrained-models.pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe391c-63c0-41b3-926f-2018fb171b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "\n",
    "class Module(pl.LightningModule):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.model = torchvision.models.resnet18()\n",
    "        self.model.fc = torch.nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        self.reset_metrics()\n",
    "        \n",
    "    def reset_metrics(self):\n",
    "        self.metrics = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def common_step(self, batch, split=\"train\"):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "        self.metrics[\"loss\"][split].append(loss.item())\n",
    "        with torch.no_grad():\n",
    "            predictions = logits.argmax(1)  # (B).\n",
    "            correct = (predictions == y).sum().item()\n",
    "            accuracy = correct / y.numel()\n",
    "            self.metrics[\"accuracy\"][split].append(accuracy)\n",
    "        return loss\n",
    "\n",
    "    # Process batch and compute the loss.\n",
    "    def training_step(self, batch):\n",
    "        return self.common_step(batch)\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        self.common_step(batch, split=\"val\")\n",
    "\n",
    "    # Logging tools.\n",
    "    def common_log(self, split=\"train\"):\n",
    "        for metric in self.metrics:\n",
    "            self.logger.experiment.add_scalars(metric, {split: np.mean(self.metrics[metric][split])}, self.global_step)\n",
    "            del self.metrics[metric][split]\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.common_log()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.common_log(split=\"val\")\n",
    "\n",
    "\n",
    "model = Module()\n",
    "data = Data()\n",
    "trainer = pl.Trainer(max_epochs=10,\n",
    "                     logger=pl.loggers.TensorBoardLogger(\"lightning_logs\", name=\"default\"))\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e365a1-7a43-4c42-9d9f-35a379321cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdaed26-741e-42b4-a36f-0b709acd190d",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "1. Validation accuracy reaches plateau, but training accuracy continues to grow.\n",
    "2. Validation loss starts to grow after a few epochs.\n",
    "3. We see *overfitting*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a14a9-e2e5-436a-b6e9-9455d3f9ab1e",
   "metadata": {},
   "source": [
    "# Augmentation with OpenCV\n",
    "\n",
    "OpenCV is an opensource library with a variety of tools for image processing and feature extraction. We will use its image processing routines.\n",
    "\n",
    "We will manually implement the following augmentations:\n",
    "1. Random horizontal flip\n",
    "2. Rotation / Scale / Offset\n",
    "3. Random crop\n",
    "4. Brightness / Contrast\n",
    "5. Blur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c75cd5-6c28-4f45-a7bc-6170c5bc10ee",
   "metadata": {},
   "source": [
    "# Assignment 1: Random horizontal flip\n",
    "\n",
    "Please, implement the transformation which flips an input image with a probability of $0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbdfcaa-3f3e-4db6-899a-326d4ba4a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlipAugmenter(object):\n",
    "    def __call__(self, image):\n",
    "        image = np.array(image)  # PIL -> Numpy.\n",
    "        h, w, c = image.shape\n",
    "        assert c == 3\n",
    "\n",
    "        # Please, flip the original image with a probability of 50%.\n",
    "        #\n",
    "        # The beggining of your code.\n",
    "        new_image = ...\n",
    "        # The end of your code.\n",
    "        \n",
    "        return Image.fromarray(new_image)  # Numpy -> PIL.\n",
    "\n",
    "show_augmenter_results(FlipAugmenter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f999514f-cafe-47cd-a479-07ad58ac9505",
   "metadata": {},
   "source": [
    "# Random rotation\n",
    "\n",
    "**Affine transformation**\n",
    "\n",
    "An affine transformation linearly moves each pixel of an image. A linear transformation is defined as:\n",
    "```\n",
    "y = Ax + b,\n",
    "```\n",
    "where $A$ is a square affine transformation matrix and $b$ is an offset. The matrix $A$ affects rotation, shear transform, and reflection. The bias vector $b$ defines an offset.\n",
    "\n",
    "For example, the following transform:\n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix},\n",
    "$\n",
    "\n",
    "$\n",
    "b = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix},\n",
    "$\n",
    "\n",
    "will swap $x$ and $y$ coordinates of each pixel, i.e. reflects an image over a diagonal, and moves an image by 2 pixels to the bottom right.\n",
    "\n",
    "Another example is scaling. The following operator scales an image by a factor of 2:\n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}.\n",
    "$\n",
    "\n",
    "**Homogeneous coordinates**\n",
    "\n",
    "In practice the both matrix $A$ and vector $b$ are stored in a single 2 x 3 matrix:\n",
    "\n",
    "$\n",
    "A' = \\begin{bmatrix} A; b \\end{bmatrix},\n",
    "$\n",
    "\n",
    "and it is assumed that an input vector contains an extra element equal to $1$:\n",
    "\n",
    "$\n",
    "x' = \\begin{bmatrix} x \\\\ 1 \\end{bmatrix}.\n",
    "$\n",
    "\n",
    "This way, both transforms are identical:\n",
    "\n",
    "$\n",
    "A x + b = A' x'\n",
    "$\n",
    "\n",
    "**Example transform with OpenCV**\n",
    "\n",
    "We use the ```warpAffine``` method to apply the affine transform to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c0e42-5239-45e9-8699-5ab20c951f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [0, 1, 2],\n",
    "    [1, 0, 2]\n",
    "]).astype(np.float32)\n",
    "image = data_module.test_dataloader().dataset.data[10]\n",
    "image2 = cv2.warpAffine(image, A, (32, 32))\n",
    "plt.imshow(np.concatenate([image, image2], 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e16c765-1a7f-4deb-a891-ba699e612905",
   "metadata": {},
   "source": [
    "# Assignment 2: Random rotation\n",
    "\n",
    "Please, fill the following block and implement a random rotation augmenter.\n",
    "\n",
    "Cheat sheet:\n",
    "```python\n",
    "cv2.getRotationMatrix2D(center, angle, scale)  # center: (x, y), angle: degrees, scale: scalar.\n",
    "\n",
    "cv2.warpAffine(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]])\n",
    "```\n",
    "\n",
    "Where `dsize` is tuple (w, h) with the desired output image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7deec73-a0ad-4629-9a1b-ed957607778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineAugmenter(object):\n",
    "    def __init__(self, max_angle=45, min_scale=0.9, max_scale=1.1, max_offset=0.1):\n",
    "        self._max_angle = max_angle\n",
    "        self._min_scale = min_scale\n",
    "        self._max_scale = max_scale\n",
    "        self._max_offset = max_offset\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        image = np.array(image)  # PIL -> Numpy.\n",
    "        h, w, c = image.shape\n",
    "        assert c == 3\n",
    "        \n",
    "        angle = random.random() * 2 * self._max_angle - self._max_angle\n",
    "        scale = self._min_scale + random.random() * (self._max_scale - self._min_scale)\n",
    "        x_offset = random.randint(-int(self._max_offset * w), int(self._max_offset * w))\n",
    "        y_offset = random.randint(-int(self._max_offset * h), int(self._max_offset * h))\n",
    "\n",
    "        # Use OpenCV to transform the image with the angle, scale, and offset, defined above.\n",
    "        # It is prefered to use the gray background.\n",
    "        #\n",
    "        # The beggining of your code.\n",
    "\n",
    "        new_image = ...\n",
    "        \n",
    "        # The end of your code.\n",
    "        \n",
    "        return Image.fromarray(new_image)  # Numpy -> PIL.\n",
    "\n",
    "\n",
    "show_augmenter_results(AffineAugmenter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cebdbb-4e8c-477f-9206-65072f6fe8f7",
   "metadata": {},
   "source": [
    "# Assignment 3: Random crop\n",
    "\n",
    "Random crop extracts a small fragment of the original image. Please, fill the following code.\n",
    "\n",
    "Cheat sheet:\n",
    "```(python)\n",
    "cv2.resize(src, dsize[, dst[, fx[, fy[, interpolation]]]])\n",
    "```\n",
    "\n",
    "Where `dsize` is tuple (w, h) with the desired output image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7b975-dcc2-46c0-8085-75abd598bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropAugmenter(object):\n",
    "    def __init__(self, min_scale=0.8):\n",
    "        self._min_scale = min_scale\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        image = np.array(image)  # PIL -> Numpy.\n",
    "        h, w, c = image.shape\n",
    "        assert c == 3\n",
    "        scale = self._min_scale + random.random() * (1 - self._min_scale)\n",
    "        new_w = int(scale * w)\n",
    "        new_h = int(scale * h)\n",
    "        x = random.randint(0, w - new_w)\n",
    "        y = random.randint(0, h - new_h)\n",
    "\n",
    "        # Please, create a fragment of the original image,\n",
    "        # defined by the offset (x, y) and size (new_w, new_h).\n",
    "        #\n",
    "        # The beggining of your code.\n",
    "\n",
    "        new_image = ...\n",
    "\n",
    "        # The end of your code.\n",
    "        \n",
    "        return Image.fromarray(new_image)  # Numpy -> PIL.\n",
    "\n",
    "show_augmenter_results(CropAugmenter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc8397-b5ff-4497-beca-a07295313877",
   "metadata": {},
   "source": [
    "# Assignment 4: Changing brightness & contrast\n",
    "\n",
    "Apply the following trasform (like in lecture slides):\n",
    "\n",
    "$\n",
    "f(x) = c (x - 128) + 128 + b\n",
    "$\n",
    "\n",
    "Keep the brightness and parameters ranges in mind. Also pay attention to data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a4e70-4148-4bc4-9001-ee5e5e961c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrightnessContrastAugmenter(object):\n",
    "    def __init__(self, brightness=0.3, contrast=0.3):\n",
    "        self._brightness = brightness\n",
    "        self._contrast = contrast\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        image = np.array(image)  # PIL -> Numpy.\n",
    "        h, w, c = image.shape\n",
    "        assert c == 3\n",
    "        brightness = 2 * (random.random() - 0.5) * self._brightness  # In the range [-1, 1].\n",
    "        contrast = 1 + 2 * (random.random() - 0.5) * self._contrast  # In the range [0, 2].\n",
    "\n",
    "        # Apply the brightness and contrast defined above.\n",
    "        #\n",
    "        # The beggining of your code.\n",
    "        \n",
    "        new_image = ...\n",
    "        \n",
    "        # The end of your code.\n",
    "        \n",
    "        assert new_image.dtype == np.uint8\n",
    "        return Image.fromarray(new_image)  # Numpy -> PIL.\n",
    "    \n",
    "show_augmenter_results(BrightnessContrastAugmenter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d490370-0caf-4754-93d2-549b32930479",
   "metadata": {},
   "source": [
    "# Assignment 5: Gaussian blur\n",
    "\n",
    "Cheat sheet:\n",
    "```(python)\n",
    "cv2.GaussianBlur(src, ksize, sigmaX[, dst[, sigmaY[, borderType]]])\n",
    "\n",
    "# ksize: (w, h)\n",
    "# sigmaX: scalar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1622e469-6694-4d2d-93ff-fac5b9cfe0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlurAugmenter(object):\n",
    "    def __init__(self, max_kernel=5):\n",
    "        self._max_kernel = max_kernel\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        kernel = random.randint(0, self._max_kernel // 2) * 2 + 1\n",
    "        if kernel == 1:\n",
    "            return image\n",
    "        image = np.array(image)  # PIL -> Numpy.\n",
    "        h, w, c = image.shape\n",
    "        assert c == 3\n",
    "        \n",
    "        # The beggining of your code.\n",
    "        \n",
    "        new_image = ...\n",
    "        \n",
    "        # The end of your code.\n",
    "        \n",
    "        return Image.fromarray(new_image)  # Numpy -> PIL.\n",
    "    \n",
    "show_augmenter_results(BlurAugmenter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a12685-4a09-4862-8fdb-b1addfaa167e",
   "metadata": {},
   "source": [
    "# Augmented training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180fdf5-9a71-4f52-b8e8-faf2da64fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAugmentation(object):\n",
    "    def __init__(self, *augmenters):\n",
    "        self._augmenters = list(augmenters)\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        augmenter = random.choice(self._augmenters)\n",
    "        return augmenter(image)\n",
    "    \n",
    "augmenter = RandomAugmentation(FlipAugmenter(),\n",
    "                               AffineAugmenter(),\n",
    "                               CropAugmenter(),\n",
    "                               BrightnessContrastAugmenter(),\n",
    "                               BlurAugmenter())\n",
    "\n",
    "show_augmenter_results(augmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8403d8b5-0232-4c69-8650-2e880524d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Module()\n",
    "data = Data(augmenter=augmenter)\n",
    "trainer = pl.Trainer(max_epochs=10,\n",
    "                     logger=pl.loggers.TensorBoardLogger(\"lightning_logs\", name=\"augmented\"))\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1c63c-b59d-4ed5-975d-a420c9b40735",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24922f52-4f5d-48b2-b419-b8e42168faa4",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "* The training set accuracy decreased and became similar to the validation quality.\n",
    "* There is no overfitting (because of BatchNorm, validation loss and accuracy are even better).\n",
    "* Validation accuracy increased, compared to the default training. Augmentations improve the final accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f4307-6e4b-49d5-a477-73cff8bbb92b",
   "metadata": {},
   "source": [
    "# Simple augmentation with Albumentations\n",
    "\n",
    "Albumentations library provides a ready-to-use set of popular image augmentations.\n",
    "\n",
    "Albumentations architecture slightly differs from torchvision:\n",
    "* In torchvision each transformation accepts an image and returns an image.\n",
    "* In albumentations each transformation accepts kwargs and returns a dictionary.\n",
    "* This way albumentations can also process labels, i.e., segmentation maps.\n",
    "* Albumentations work with Numpy tensors rather than PIL images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b81d5-f3c2-43f7-a04b-efa9d905914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "\n",
    "augmenter = albumentations.Compose([\n",
    "    albumentations.ShiftScaleRotate(rotate_limit=0.25, p=0.7),\n",
    "    albumentations.RandomBrightnessContrast(p=0.4),\n",
    "    albumentations.RandomGamma(p=0.4),\n",
    "    albumentations.Blur(blur_limit=3, p=0.1),\n",
    "    albumentations.GaussNoise((10, 100), p=0.2),\n",
    "    albumentations.HorizontalFlip(p=0.5)\n",
    "])\n",
    "\n",
    "show_augmenter_results(lambda image: augmenter(image=np.array(image))[\"image\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ebabd4-f3f0-411e-a65c-e164f356e562",
   "metadata": {},
   "source": [
    "# Homework (optional)\n",
    "* Try to add the Gamma correction augmenter and additive noise\n",
    "* Adjust hyperparameters to achieve a better test set quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
