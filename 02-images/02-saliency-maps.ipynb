{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5221e2b8-abe4-4afe-889a-2765f4121f77",
   "metadata": {},
   "source": [
    "# Saliency maps\n",
    "\n",
    "An important tool in debugging CV models is a saliency map. A saliency map indicates regions of interest of an input image. There are two main techniques for convolutional models: input gradient and GradCAM.\n",
    "\n",
    "**Goal.** The goal of this notebook is to develop the basic skills in model analysis and give a deeper understanding of CNN models.\n",
    "\n",
    "You need the following extra libraries beyond PyTorch:\n",
    "* torchvision\n",
    "* Pillow (PIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283986b2-1ddf-43a4-a935-480e5e253ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.cm import jet\n",
    "\n",
    "# Helper tools. You can skip this block.\n",
    "def show_map(x, saliency):\n",
    "    assert x.ndim == 3 and saliency.ndim == 2\n",
    "    image = x.detach().clone().permute(1, 2, 0)\n",
    "    image -= image.min()\n",
    "    image /= image.max()\n",
    "\n",
    "    s = saliency.detach().clone()\n",
    "    if s.shape != image.shape[:2]:\n",
    "        s = torch.nn.functional.interpolate(s[None, None], image.shape[:2], mode=\"bilinear\")[0, 0]\n",
    "    s -= s.min()\n",
    "    s /= s.max()\n",
    "    s = jet(s)[:, :, :3]  # Remove alpha channel.\n",
    "    s = torch.as_tensor(s)  # (H, W, C).\n",
    "\n",
    "    mixed = 0.5 * image + 0.5 * s\n",
    "    print(x.shape, s.shape, mixed.shape)\n",
    "    plt.imshow(torch.cat([image, s], 1))\n",
    "    plt.show()\n",
    "    plt.imshow(mixed)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77ec2d-d9ca-4dbd-95c7-46f725e70c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download the image.\n",
    "#! wget -O image.jpg https://raw.github.com/ivan-chai/isscai-cv-2024/master/02-images/image.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c88c49-a40f-4d04-a410-ed11066c0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained ResNet34 model.\n",
    "weights = torchvision.models.ResNet34_Weights.IMAGENET1K_V1\n",
    "model = torchvision.models.resnet34(weights=weights).eval()\n",
    "\n",
    "# Load the image.\n",
    "image = Image.open(\"image.jpg\")\n",
    "plt.title(\"Original image\")\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "# Preprocess.\n",
    "x = weights.transforms()(image)\n",
    "print(\"Preprocessed image size:\", x.shape)\n",
    "plt.title(\"Preprocessed\")\n",
    "preprocessed = x.clone().permute(1, 2, 0)\n",
    "preprocessed -= preprocessed.min()\n",
    "preprocessed /= preprocessed.max()\n",
    "plt.imshow(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee945362-287f-4f6a-a312-ea282032c5c2",
   "metadata": {},
   "source": [
    "# Recognition\n",
    "\n",
    "The model was pretrained on the ImageNet 1K dataset and capapable of recognizing both ships and cats. Let's show top predicted classes.\n",
    "\n",
    "**Load label names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa658b-2b47-4934-8514-b6df7b3b0d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt -O imagenet1000_clsidx_to_labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128a144-128d-4903-9e72-6945ec654e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"imagenet1000_clsidx_to_labels.txt\") as fp:\n",
    "    labels = yaml.safe_load(fp)\n",
    "print(\"Num classes:\", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcc1a3f-0743-4714-90de-a449ca86b8b0",
   "metadata": {},
   "source": [
    "**Show predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f9164-0195-4d85-a602-e69d20bc3ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(x[None])[0]\n",
    "probs = torch.nn.functional.softmax(logits, dim=0)\n",
    "top = probs.argsort(descending=True)[:10].tolist()\n",
    "for i in top:\n",
    "    print(f\"Class {i} {labels[i]}, prob: {probs[i] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809f844-740b-4356-ae2d-beb6174a7d7d",
   "metadata": {},
   "source": [
    "**Discussion.** We see a mixture of ship and cat / tiger classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b230b406-ae17-4901-bf68-2a7359a25367",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIP = 628\n",
    "CAT = 282"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c68a504-1636-4ab0-a4c1-6958138e4da4",
   "metadata": {},
   "source": [
    "# Simple gradient\n",
    "\n",
    "The basic algorithm to measure the impact of an image region on the model output is to just compute the gradient value w.r.t. the input image. A large gradient magnitude indicates an important region (output is sensitive w.r.t. to this region). Otherwise, zero gradient means a small perturbation of this region will not affect the output.\n",
    "\n",
    "Gradient can be easily computed using a standard PyTorch Autograd algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d300d8-5ee6-462b-8e25-825b08c17bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad = True\n",
    "x.grad = None\n",
    "logits = model(x[None])[0]\n",
    "logits[SHIP].backward()\n",
    "print(\"Gradient shape:\", x.grad.shape)\n",
    "print(f\"Gradient range: [{x.grad.min().item():.2f}, {x.grad.max().item():.2f}]\")\n",
    "\n",
    "# Compute gradient norm for each pixel.\n",
    "# We need clipping to measure the positive impact only.\n",
    "value = torch.linalg.norm(x.grad.clip(min=0), dim=0)  # (C, H, W) -> (H, W).\n",
    "plt.title(f\"Saliency map for '{labels[SHIP]}'\")\n",
    "show_map(x, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a46f7-2ce2-434b-9a64-215a6822cecd",
   "metadata": {},
   "source": [
    "**Discussion.**\n",
    "* The saliency map is noisy and unstable.\n",
    "* The logit responsible for the ship class ignores the region with the cat.\n",
    "* The model looks at the bottom of the ship and at the sea above.\n",
    "* The decision of the model can be affected by modifying a little amount of pixels in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063dda7-abf3-4182-ba46-8706bd304020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient norm for each pixel.\n",
    "x.requires_grad = True\n",
    "x.grad = None\n",
    "logits = model(x[None])[0]\n",
    "logits[CAT].backward()\n",
    "value = torch.linalg.norm(x.grad.clip(min=0), dim=0)  # (C, H, W) -> (H, W).\n",
    "plt.title(f\"Saliency map for '{labels[CAT]}'\")\n",
    "show_map(x, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7159fc21-03d2-4e84-af47-f7a2fb29bef2",
   "metadata": {},
   "source": [
    "**Discussion.**\n",
    "* The saliency map is also noisy and unstable.\n",
    "* The model looks at the area nearby the cat.\n",
    "* The decision of the model can be affected by modifying pixels in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a92d02-a2fd-46fc-a4e2-bb7ec9673765",
   "metadata": {},
   "source": [
    "# GradCAM\n",
    "\n",
    "GradCAM exploits the structure of a CNN model:\n",
    "* Before the global pooling layer (highlighted) we have a feature map for each region in the image.\n",
    "* We can apply a classification head to each feature vector (out of 7 x 7) and generate class probability.\n",
    "* This probability indicates the correspondence of the region to the class.\n",
    "\n",
    "We will implement a simple method similar to GradCAM.\n",
    "\n",
    "<img src=\"vgg16.jpg\" width=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330146d8-a774-4c42-bbc7-bb69437c1353",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x[None]\n",
    "y = model.conv1(y)\n",
    "y = model.bn1(y)\n",
    "y = model.relu(y)\n",
    "y = model.maxpool(y)\n",
    "y = model.layer1(y)\n",
    "y = model.layer2(y)\n",
    "y = model.layer3(y)\n",
    "activations = model.layer4(y)[0]\n",
    "print(\"Activations shape:\", activations.shape)\n",
    "d, h, w = activations.shape\n",
    "y = activations.flatten(1).T  # (HW, C).\n",
    "y = model.fc(y).reshape(h, w, -1)  # (H, W, C).\n",
    "probs = torch.nn.functional.softmax(y, dim=-1)  # (H, W, C).\n",
    "print(\"Probabilities shape:\", probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033eb77-f3c3-4eb5-8a7b-20e5d25ea4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(f\"GradCAM map for '{labels[top[0]]}'\")\n",
    "show_map(x, probs[..., top[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130277e9-b085-423f-a2e4-7721c53342a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(f\"GradCAM map for '{labels[top[1]]}'\")\n",
    "show_map(x, probs[..., top[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da9809-7d7f-4c16-9cea-075c41bfdd1a",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "* GradCAM is more stable.\n",
    "* GradCAM has little granularity.\n",
    "* GradCAM can also be used for **object detection**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8362bdb-7475-43df-bcd7-bf29b8c516a6",
   "metadata": {},
   "source": [
    "# Home work (optional)\n",
    "\n",
    "See the original [GradCAM work](https://arxiv.org/pdf/1610.02391).\n",
    "* What are the differences between our approach and GradCAM?\n",
    "* Try to implement the original GradCAM approach. Are there any visible differences in the result?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
